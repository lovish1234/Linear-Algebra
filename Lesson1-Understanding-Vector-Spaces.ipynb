{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ##                                                           Linear Algebra for Machine Learning - ARL 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is linear algebra ?\n",
    "\n",
    "There are many perspectives to what Linear Algebra is :-\n",
    "\n",
    "- Study of certain algebric structures  called vector spaces\n",
    "- Study of linear sets of equations and their properties\n",
    "- Field of mathematics related to finite dimensional vector spaces and linear mappings between such spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should we study linear algebra at first place ?\n",
    "\n",
    "It is very important for Machine Learning practitionars to understand the underlying operations performed by standard liberaries such as numpy, torch etc. Most of these liberaries operate on objects called matrix or tensor and are optimised for speed and numerical stability. Data science is all about maniuplating matrices since majority of the data we use can be represented easily in matrix form. This involves structured data, time-series, language (word embeddings), images etc.\n",
    "\n",
    "During fresher/sophomore year in the engineering curriculum, linear algebra is taught in a very abstract manner. Majority of ML practitioners can barely relate directly to the taught objects directly as the methods for solving large matrix problems via a computer is drastically different in many ways. \n",
    "\n",
    "First, speed of computations tend to be slower when big matrices are involved. There are different ways to address this issue such that vectorizing the code, faster algorithms, memory optimisation methods.\n",
    "\n",
    "Secondly, computer represents numbers in a discrete and finite way, means that they have limited accuracy. So, if you are iterating by multiplying a matrix again and again, rounding errors can add up. Moreover, some of the operations are not numerically stable meaaning that a small change in input causes a significant change in the output.\n",
    "\n",
    "Finally, matrices which are sparse may end up consuming lot of memory if they are not stored in an efficient manner. These include upper traingular, lower triangular and symmetric matrices.\n",
    "\n",
    "Consequently, we need to be aware of the issues which may cause our ML system to default because of accuracy/stability/memory space used by the matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief History of linear algebra\n",
    "\n",
    "- Around 4000 years ago, the Babylons knew how to solve two equations with two variables. Around 200 B.C, Chinese published a work titled 'Nine Chapters of Mathematical Art' which contained a method to solve 3x3 linear equations.\n",
    "- But the real progress started in $17^{th}$ century, when something we call 'determinant' was studied by Liebniz\n",
    "\n",
    "<img src=\"./images/chap1/x.png\" width=200 height=100 >\n",
    "\n",
    "\n",
    "- Simultaneously, Lagrange started work on finding maxima and minima of multivariate functions which we know today as 'Lagrange's multiplier'\n",
    "\n",
    "<img src=\"./images/chap1/y.jpeg\" width=200 height=100 >\n",
    "\n",
    "- In the start of $18^{th}$ century, Cramer came up with the fourmula to solve system of linear equations using determinant. \n",
    "\n",
    "- With the advent and use of calculus, linear algebra became more relevant. Euler and then Gauss worked on set of linear equations in which number of variables might differ from the number of variables. A popular stratagy that Gauss invented to solve these set of equations is known as 'Gaussian elimination'\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/chap1/e.jpg\" width=200 height=100 >\n",
    "<img src=\"./images/chap1/g.jpg\" width=200 height=100 >\n",
    "</div>\n",
    "\n",
    "- Introduction of matrix notation and the invention of the word matrix were motivated by attempts to develop a language to learn about determinants. JJ Sylvester named the notation 'matrix' as it means 'womb' in Latin, a womb out of which determinant is born.\n",
    "\n",
    "- Caylay defined the matrix multiplicaiton as composition of 'two linear transformations is vector space'. He also introduced the idea of identity matrix and the inverse matrix. His work culminated in Caylay-hamilton theorm which states that every matrix satisfies it's characterstic equation.\n",
    "\n",
    "- During the late 19 and early $20^{th}$ century, linear algebra was used to solve problems in physics and more attention was given to vectors as they seemed to provide basic geometrical interpretation to the mathematical elements in linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Courses on Linear Algebra\n",
    "\n",
    "There are plethora of online courses available on Linear Algebra. Some of them already offer an excellent overview of LA from a ML perspective. \n",
    "\n",
    "- __[Computational Linear Algebra by Fast.ai](http://www.fast.ai/2017/07/17/num-lin-alg/)__\n",
    "- __[Linear Algebra by Gilbert Strang](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/)__\n",
    "\n",
    "Additional resources include short courses that help have a better 'feel' of some topics.\n",
    "\n",
    "- __[Essence of Linear Algebra by Grant Sanderson ](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)__\n",
    "- __[Immersive Linear Algebra by J. Strom et al](http://immersivemath.com/ila/index.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charter of this course\n",
    "\n",
    "Following are the core values of the course.\n",
    "\n",
    "TLDR ; It is a tweaked ensable of the courses mentioned above. \n",
    "\n",
    "- Learn through visualization and examples\n",
    "- Learn linear algebra to be comfortable with Machine Learning\n",
    "- Broach upon abstractions which we sometimes encounter in technical papers\n",
    "- How can we do matrix computations with acceptable speed and accuracy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllabus of this course\n",
    "\n",
    "\n",
    "- Scalar, Vectors & Tensors \n",
    "- Vector Spaces & related aspects\n",
    "- Determinant of the Matrix\n",
    "- Inverse of a linear transormation or Matrix\n",
    "- Eigenvalues and Eigenvectors \n",
    "- The \"Symmetric\" Matrix\n",
    "- Singular Value Decomposition\n",
    "- Pseudo-Inverse of a Matrix & it's applications\n",
    "- Principal Component Analysis \n",
    "- Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventions of this course\n",
    "\n",
    "Conventions used in the course are same as used in Deep Learning book by Deep Learning book\n",
    "\n",
    "- scalars are written in lowecase and *italics*. \n",
    "- vectors are written in lowercase and **bold**\n",
    "- Matrices are written in Uppercase and **bold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Some of the common operations that we are already introduced to in high school or during freshmen year are :-\n",
    "\n",
    "**Matrix Multiplication**\n",
    "\n",
    "$$\n",
    "\\textbf{M}=\n",
    "  \\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\n",
    "  \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    e & f\\\\\n",
    "    g & h\n",
    "  \\end{bmatrix} =\n",
    "  \\begin{bmatrix}\n",
    "    ae+bg & af+bh\\\\\n",
    "    ce+dg & cf+dh\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Determinant**\n",
    "\n",
    "$$\n",
    "\\textbf{D} = det(\n",
    "  \\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\n",
    "  \\end{bmatrix}\n",
    ") = (ad - bc)\n",
    "$$\n",
    "\n",
    "**Cross Product**\n",
    "\n",
    "$$\n",
    "\\textbf{C} = det(\n",
    "  \\begin{bmatrix}\n",
    "    \\hat{i} & \\hat{j} & \\hat{k} \\\\\n",
    "    a & b & c \\\\\n",
    "    d & e & f\n",
    "  \\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "** Eigenvalues **\n",
    "\n",
    "$$\\textbf{E} = det(\\textbf{A} - \\lambda \\textbf{I}) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this lecture series, let's try to get a feel of why matrix multiplication is defined the way it is ? Why does cross product has anything to do with determinant of the matrix ? What does \"eigen\" or \"singular\" value mean ?\n",
    "\n",
    "The practical material included in this course encompasses some techniques to\n",
    "\n",
    "- limit memory use\n",
    "- increase speed\n",
    "- enable parallalization\n",
    "\n",
    "for matrix computations. As you might have guessed, often there is a trade-off between the above points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcomes\n",
    "\n",
    "Some things which you should be able to answer are :-\n",
    "\n",
    "- Calculate Rank of a matrix \n",
    "- Row Echelon Form / Reduced Row Echelon Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with a generalized view of what a vector space is and then we are going to narrow down to which defination we are going to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups \n",
    "\n",
    "Set of elements and an operation defined on these elements such that keeps some structure of the original set intact. Formally, we can define a group as :-\n",
    "\n",
    "Consider a group $\\mathcal{G}$ and an operation defined on this group $\\otimes : \\mathcal{G} \\rightarrow \\mathcal{G}$. Then $\\mathcal{G}$ is called a group if the following holds :-\n",
    "\n",
    "- **Closure** - Clousure of $\\mathcal{G}$ under $\\otimes$ ie $\\forall x,y \\in \\mathcal{G}  \\hspace{0.1in} x \\otimes y \\in \\mathcal{G}$ \n",
    "- **Associativity** - $\\forall x,y,z \\in \\mathcal{G} \\hspace{0.1in} (x \\otimes y)\\otimes z = x \\otimes (y \\otimes z)$\n",
    "- **Identity/Neutral Element** - $\\exists \\hspace{0.1in} I \\in \\mathcal{G} \\hspace{0.1in} x \\otimes I = x$\n",
    "- **Inverse Element** - $\\forall x \\in \\mathcal{G} \\hspace{0.1in} \\exists \\hspace{0.1in} x \\otimes y = I $\n",
    "\n",
    "If you have understood it correctly, let's start with some hands on practice :-\n",
    "\n",
    "Is X a group ?\n",
    "\n",
    "- $(\\mathcal{Z}, +)$ where $\\mathcal{Z}$ is the set of complex numbers\n",
    "- $(\\mathcal{N}, +)$ where $\\mathcal{N}$ is the set of natural numbers \n",
    "- $(\\mathcal{N} \\cup 0, +)$ where $\\mathcal{N}$ is the set of natural numbers \n",
    "- $(\\mathcal{Z}, .)$ where $\\mathcal{Z}$ is the set of complex numbers\n",
    "- $(\\mathcal{R}, .)$ where $\\mathcal{R}$ is the set of real numbers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reviewing groups, we had a look at set $\\mathcal{G}$ and operations such that $\\mathcal{G} \\otimes \\mathcal{G} \\rightarrow \\mathcal{G}$. Now, let's consider a set $\\mathcal{V}$ which in addition to the inner operation $+$, we also have an outer operation $.$\n",
    "\n",
    "**What is a vector space ?**\n",
    "\n",
    "Set $\\mathcal{V}$ is called a real valued vector space if \n",
    "\n",
    "- $(\\mathcal{V},+)$ is a group\n",
    "- Distributivity wrt outer operation\n",
    "    - $\\lambda.(x + y) = (\\lambda.x + \\lambda.y) \\hspace{0.1in} \\forall \\lambda \\in \\mathcal{R} \\forall x,y \\in \\mathcal{V} $ \n",
    "    - $(\\lambda + \\phi).x = \\lambda.x + \\phi.y \\hspace{0.1in} \\forall \\lambda, \\phi \\in \\mathcal{R} \\forall x \\in \\mathcal{V}$\n",
    "- Associativity wrt outer operation\n",
    "    - $\\lambda.(\\phi.x) = \\lambda\\phi.(x) \\hspace{0.1in} \\forall \\lambda, \\phi \\in \\mathcal{R} \\forall x \\in \\mathcal{V}$\n",
    "- Neutral element wrt outer operation\n",
    "    - $I.x = x \\hspace{0.1in} \\forall x \\in \\mathcal{R}$\n",
    "    \n",
    "What is the identity element for group $\\mathcal{V}$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the addition and multiplication on a vector space $\\mathcal{V}$\n",
    "\n",
    "$\\mathcal{V} = \\mathcal{R}^{n}, n \\in \\mathcal{N}$ is a vector space with the operations defined as below :-\n",
    "- Addition  \n",
    "    - $ (x+y) =  (x_{1}+x_{2}+x_{3}+\\cdots+x_{n}) + (y_{1}+y_{2}+y_{3}+\\cdots+y_{n})  = (x_{1}+y_{1}) +(x_{2}+y_{2})+(x_{3}+y_{3})+\\cdots+(x_{n}+y_{n})  $\n",
    "- Scalar Multiplication \n",
    "    - $\\lambda.x = \\lambda(x_{1}+x_{2}+x_{3}+\\cdots+x_{n}) = (\\lambda.x_{1}+\\lambda.x_{2}+\\lambda.x_{3}+\\cdots+\\lambda.x_{n}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is vector subspace ?**\n",
    "\n",
    "A subspace of original vector space such that when we perform operations on elements of subspace, then we will never leave it. In this sense, we can say that they are closed under operations\n",
    "\n",
    "Formally, let $(\\mathcal{V},+,.)$ be a $\\mathcal{R}$-vector space and $\\mathcal{U} \\subseteq \\mathcal{V}$, $\\mathcal{U} \\neq \\phi$ then $\\mathcal{U}$ is called the vector subspace of $\\mathcal{V}$ if the following are satisfied \n",
    "\n",
    "- $\\mathcal{U} \\times \\mathcal{U} \\rightarrow \\mathcal{U}$\n",
    "- $\\mathcal{R} \\times \\mathcal{U} \\rightarrow \\mathcal{U}$\n",
    "\n",
    "How to check if $\\mathcal{U}$ is a vector subspace of $\\mathcal{V}$ ?\n",
    "\n",
    "Some of the properties of $\\mathcal{V}$ are inherited \"as is\" by $\\mathcal{U}$. This includes the properties that ($\\mathcal{U}$,+) is a group, associativity, distributivity and neutral element with respect to \".\" operation. However, we still need to check two things before we can conclude that it's a \"subspace\" \n",
    "\n",
    "- Closure of $\\mathcal{U}$ wrt outer operation . ie $\\forall \\lambda \\in \\mathcal{R} \\hspace{0.1in} \\forall x \\in \\mathcal{U} : \\lambda .x \\in \\mathcal{U}$\n",
    "- Closure of $\\mathcal{U}$ wrt inner operation + ie $\\forall x,y \\in \\mathcal{U} \\hspace{0.1in} x+y \\in \\mathcal{U}$\n",
    "- Presence of Identity element in $\\mathcal{U}$\n",
    "\n",
    "\n",
    "Is X a vector subspace of Y ?\n",
    "\n",
    "In the below figure, let's say that $\\mathcal{R}^{2}$ is the vector space. Is the blue area vector subspace of $\\mathcal{R}^{2}$ ?\n",
    "\n",
    "<img src=\"./images/chap1/Vector_Subspace.png\" width=600 height=300 >\n",
    "\n",
    "\n",
    "Let's take a more interesting example, prove that :-\n",
    "\n",
    "- The solution set of equation $A.x = 0$ with n unknowns is a subspace of $\\mathcal{R}^{n}$\n",
    "- The solution set of equation $A.x = b$ where $b \\neq 0$ is not a subspace of $\\mathcal{R}^{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whis is linear dependence/ linear independence ?**\n",
    "\n",
    "The discussion on vector spaces made it clear that we can add two vectors or multiply them with scalar. However, after we conduct either or both of these operations, we would end up with a vector in the same vector space.\n",
    "\n",
    "- **Linear combination of vectors** - Consider a vector space $\\mathcal{V}$ and finite number of elements in $\\mathcal{V} :\\hspace{0.1in} (x_{1},x_{2}, \\cdots x_{n})$ called vectors. Then every vector $\\mathcal{v}$ can be represented as :-\n",
    "\n",
    "$$ \\mathcal{v} = \\lambda_{1}x_{1}+\\lambda_{2}x_{2}+\\cdots+\\lambda_{n}x_{n} = \\sum_{i=1}^{n}\\lambda_{i}x_{i} \\in \\mathcal{V}$$\n",
    "\n",
    "If $\\mathcal{v} = 0$, there always exists a trivial solution ie $\\forall \\lambda_{i} : \\lambda_{i}=0$. However, we are interested in finding non-trivial solution to the equation ie $\\exists \\lambda_{i} : \\lambda_{i} \\neq 0$. If that happens, vectors $(x_{1}, x_{2}, \\cdots, x_{n})$ are known to be **linearly dependent**. Otherwise they are **linearly independent**\n",
    "\n",
    "Linear independent is one of the most important concepts in linear algebra. Intutively, set of linearly independent vectors are vectors which have no redundancy ie. if any of the vector is removed from the set, we will loose something. Let us formalize this intuition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the practical way of checking if set of vectors are linearly independent/dependent ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is basis ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is rank ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start...\n",
    "\n",
    "There are a few ways of of looking at a vector. The first and the most \"intuitive\" is to imagine an arrow in space. This is how we interpret vectors in real world eg. with a magnitude and direction. We call it intuitive as geometrical space will help us in getting an insight into many of operations in linear algebra which may seem rather abstract otherwise.\n",
    "\n",
    "<img src=\"images/chap1/arrow.png\" height=\"100\" width=\"100\" >\n",
    "\n",
    "The second way is to look at a vector is to look at it as a list of numbers. This is essentialy useful in numerical computations.\n",
    "\n",
    "$$ V = \n",
    "  \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    3\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In a more abstract way, a vector can also be written as $\\hat{v}$. \n",
    "\n",
    "More generally, the theory of linear algebra is easy to appreciate through the interplay of first two perspectives ie arrow space. \n",
    "\n",
    "<img src=\"images/chap1/2WayVec.png\" height=\"100\" width=\"150\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Algebra** is a branch of mathematics that deals with linear equations.\n",
    "\n",
    "The following is an example of a linear equantion :-\n",
    "\n",
    "$a_{1}x_{1}+ \\cdots +a_{n}x_{n} = b$\n",
    "\n",
    "The LHS in the above equation can be written as a transformation of $(x_{1},\\cdots,x_{n})$ to $(a_{1}x_{1}+\\cdots+a_{n}x_{n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief History of Linear Algebra \n",
    "\n",
    "Initially, the field started from the introduction of determinents for solving the system of linear equations. \n",
    "\n",
    "Liebniz --> Cramer --> Gauss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a vector space ?\n",
    "\n",
    "One major idea in mathematics is of finding a \"closure\" set of operations. What is the set of objects which we can obtain after certain operations on objects ? In our case, what would be the possible set of vectors that can be obtained starting from a small set of vectors after :-\n",
    "\n",
    "- Adding them up\n",
    "- Scaling the result with a particular value\n",
    "\n",
    "This concept of vector space forms a basis of much of machine learning research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the basis of a vector space ?**\n",
    "\n",
    "Something which defines the entire coordinate space.\n",
    "\n",
    "Set of **linearly independent** vectors that span that space.\n",
    "\n",
    "You can change the basis vectors from the standard basis $\\hat{i}$ and $\\hat{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What is the linear combination of two vectors ? **\n",
    "\n",
    "It consits of two different properties of the vectors. Additivity and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What is the span of two vectors $\\hat{v}$ and $\\hat{w}$ ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What is linear dependence ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a linear transformation ?\n",
    "\n",
    "To transform the vector space such that it preserves **certain properties** of the vector space structure.\n",
    "\n",
    "T : V $\\rightarrow$ W\n",
    "\n",
    "- $\\vec{0} \\rightarrow \\vec{0}$ ie. the null vector corresponding to vector space V should be mapped to the null vector in vector space W\n",
    "- The lines which are parallel in vector space V should remain parallel in the vector space W\n",
    "\n",
    "##### Geometric significance of linear transofmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Identity Matrix\n",
    "\n",
    "The identity matrix $I_{n}$. For example \n",
    "\n",
    "$$ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isomorphic Vector spaces\n",
    "\n",
    "When a bijective mapping exists between two vector spaces V and W ie every vector in W has **exactly** one vector in V associated with it, we call them isomorphic. \n",
    "\n",
    "##### How to check if two vector spaces are isomorphic ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null space (Kernel) of a matrix \n",
    "\n",
    "If the mapping is not isomorphic, we are interested in the range of elements in V what get mapped to 0 in W. We call this the null space of T\n",
    "\n",
    "If the determinant is non-zero, then the null space is trivial to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subspaces, Basis and Span \n",
    "\n",
    "Both the range and kernel of a linear subspace is a vector space. A simple way to define a subspace is take a set of vectors $(v_{1}, v_{2}, v_{3} \\cdots, v_{n})$. \n",
    "\n",
    "$$a_{1}v_{1}+a_{2}v_{2}+\\cdots+a_{n}v_{n}$$\n",
    "\n",
    "where $a_{1}, a_{2}, \\cdots , a_{n}$ are scalars. The set of linear combinations of vectors $(v_{1}, v_{2}, v_{3} \\cdots, v_{n})$ is called subspace. \n",
    "\n",
    "A linear combination of any system of vectors with all zero coefficients is the **zero vector** of V. If this is the only way to express the zero vector as a linear combination of $(v_{1}, v_{2}, ..., v_{k})$ then these vectors are linearly independent.\n",
    "\n",
    "If we are given a set of vectors that span a subspace, if any vector w is a linear comvination of other vectors then we may also remove the vector w without affecting the span. Thus, we can say that a linearly dependent subset of vectors is redundant in the sense that we can span the same subspace after removing one (or more) of the vectors. \n",
    "\n",
    "We call a set of linerly independent vectors that span a subspace V as the **basis** of V. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvectors and Eigenvalues \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What is vector in an abstract sense ? **\n",
    "\n",
    "It is an object which has two properties :-\n",
    "\n",
    "- It can be added together with another object to get an object of same kind\n",
    "- It can be multiplied with a scalar and the resultant object is of the same kind\n",
    "\n",
    "As per this defination even polynomials are vectors in an abstract sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/chap1/Linear_Algebra_Wireframe.png\" width=600 height=300 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "[1] __[Wikipedia-Linear Algebra](https://en.wikipedia.org/wiki/Linear_algebra)__\n",
    "\n",
    "[2] __[3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)__\n",
    "\n",
    "[3] __[Deep Learning by Goodfellow et al - Chapter 2 ](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-Introduction/)__\n",
    "\n",
    "[4] __[Mathematics for Machine Learning](https://mml-book.github.io/book/chapter02.pdf)__\n",
    "\n",
    "[5] __[History of Linear Algebra](http://www.math.utah.edu/~gustafso/s2012/2270/web-projects/christensen-HistoryLinearAlgebra.pdf)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
