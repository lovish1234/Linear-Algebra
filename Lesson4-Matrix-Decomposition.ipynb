{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that a linear equation cannot have two solutions. (Eigher one or infinitely many solutions) ??\n",
    "\n",
    "Think of the columns of $\\textbf{A}$ as the different directions we can travel to from origin, then determine how many different ways we can follow to reach $\\textbf{b}$.\n",
    "\n",
    "$$ \\textbf{A}\\textbf{x} = \\sum_{i}x_{i}\\textbf{A}_{:,i}$$\n",
    "\n",
    "If $\\textbf{b}$ is in the span of the coumns of $\\textbf{A}$, then $\\textbf{A}\\textbf{x}=\\textbf{b}$ must have  a solution. In this particular case, the span is called the range or **column space** of $\\textbf{A}$.\n",
    "\n",
    "The requirement that column space of $\\textbf{A}$ be all of $\\mathbb{R}^{m}$ implies that $\\textbf{A}$ must have atleast m columns. Otherwise dimensionalty of column space would be less than m. A an example take $\\textbf{A}$ as a $3 \\times 2$ matrix, that means we only have two $x_{i}$'s which can vary to produce the output. The output will lie on a 2-D plane in the 3-D space. However, this is necessary and not sufficient condition for $\\textbf{A}\\textbf{x}=\\textbf{b}$ having a solution.\n",
    "\n",
    "Even if the matrix has 3 columns, if two of them are the same, output for $\\textbf{A}\\textbf{x}$ would lie on $\\mathbb{R}^{2}$. Thus the columns should also be linearly independent.\n",
    "\n",
    "For the matrix $\\textbf{A}$ to have inverse, it is required that the equation has exactly one solution for each value of $\\textbf{b}$. Thus the matrix should have **m linearly independent columns**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norms are the set of functions mapping vectros to a set of non-negative values. The norm of a vector $\\textbf{x}$ measures the distance between origin to the point x in the space.\n",
    "\n",
    "$$ ||\\textbf{x}||_{p} = \\Big(\\sum_{i}|x_{i}|^{p}\\Big)^{\\frac{1}{p}}$$\n",
    "\n",
    "Formally, norm us any function that satisfies the following properties :-\n",
    "\n",
    "- f(**x**)=0 $\\implies$ x=0\n",
    "- f(**x**+**y**) $\\leq$ f(**x**)+f(**y**) (triangle inequality)\n",
    "- $\\forall \\alpha \\in \\mathbb{R}$, f($\\alpha$**x**)=$\\alpha$f(**x**) \n",
    "\n",
    "Most common norm is the $L^{2}(\\textbf{x})$ norm, also denoted as $||\\textbf{x}||$. $L^{2}(\\textbf{x})$ norm grows very slowly near the origin, hence when we need the distance value to increase quickly near the origin we use $L^{1}(\\textbf{x})$ norm. This is very important for some problems in machine learning where we need to differnciate the elements which are exactly 0 with other which are near to 0. $L^{1}(\\textbf{x})$ norm grows at the same rate in all the locations. \n",
    "\n",
    "There are other commonly used norms defined as following :-\n",
    "\n",
    "- $L^{0}(\\textbf{x})$ norm - Number of non-zero elements in a vector $\\textbf{x}$. Actually this violates the third requirement of being a norm. Still, it is mis-named as a norm.\n",
    "- $L^{\\infty}(\\textbf{x})$ norm - Max element in the vector $\\textbf{x}$\n",
    "$$||\\textbf{x}||_{\\infty} = max_{i} |x_{i}| $$\n",
    "- Frobenius Norm - $L^{2}$ norm with a martix $\\textbf{A}$ instead of a vector $\\textbf{x}$\n",
    "$$ ||\\textbf{A}||_{2} = \\sqrt{\\Big(\\sum_{i}\\sum_{j}|A_{i,j}|^{2}\\Big)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Kind of Matrices \n",
    "\n",
    "#### Upper Triangular Matrix\n",
    "\n",
    "#### Lower Triangular Matrix\n",
    "\n",
    "#### Diagonal Matrices\n",
    "\n",
    "A diagonal matrix, written as $\\textbf{D}$ is a matrix in which the individual entries are defined as follows :-\n",
    "\n",
    "$$\\textbf{D}_{i,j} \\neq 0 \\hspace{0.1in} \\forall i,j : i\\neq j$$\n",
    "$$\\textbf{D}_{i,j} = 0 \\hspace{0.1in} \\forall i,j : i=j$$\n",
    "\n",
    "This defination leads to many interesting properties as $\\textbf{D}\\textbf{x} = \\textbf{D}\\odot\\textbf{x}$. This means multiplying a diagonal matrix with another vector just requires the pointwise multiplication of the corresponding elements. Also, inverting a diagonal matrix requries less number of operations as \n",
    "\n",
    "$$\\textbf{D}^{-1} = \\begin{bmatrix} \\frac{1}{d_{1}} & \\dotsc & \\dotsc  & \\dotsc \\\\ \\dotsc & \\frac{1}{d_{2}} & \\dotsc  & \\dotsc \\\\ \\dotsc & \\dotsc & \\frac{1}{d_{i}} & \\dotsc \\\\ \\dotsc & \\dotsc & \\dotsc  & \\frac{1}{d_{n}} \\end{bmatrix}$$\n",
    "\n",
    "Diagnomal matrix need not be square. Although, non-square diagonal matrix does not have inverse, a multiplication by these matrices is still cheap. If number of rows is greater than number of columns :-\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 0  \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "concatenate zeros to the result. If the number of columns are greater than number of rows :-\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0  \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix} = \\begin{bmatrix} x_{1} \\\\ x_{2}  \\end{bmatrix}$$\n",
    "\n",
    "discard last elements of $\\textbf{x}$\n",
    "\n",
    "\n",
    "\n",
    "In Machine Learning, restricting some matrices to daigonal may result in a more efficient algorithm\n",
    "\n",
    "#### Symmetric Matrix\n",
    "\n",
    "A matrix which is equivalent to it's own tranpose is called symmetric matrix. \n",
    "\n",
    "$$ \\textbf{A} = \\textbf{A}^{T}$$\n",
    "\n",
    "In machine learning, symmetric matrix appear when it's entries are generated by the function having two arguments in which order of the arguments have no effect on the output of the function eg. distance metric\n",
    "\n",
    "#### Skew-Symmytric Matrix \n",
    "\n",
    "\n",
    "#### Orthogonal/Orthonormal Matrices \n",
    "\n",
    "A vector $\\textbf{x}$ and $\\textbf{y}$ are called orthogonal if $\\textbf{x}^{T}\\textbf{y}=0$. If both of these vectors have non-zero norm, it means that they are at 90 degrees angle to each other. If both of the vectors have a unit norm, they are called **orthonormal**.\n",
    "\n",
    "Orthonormal matrix is a square matrix in which both the rows and columns are mutually orthonormal to each other. For this reason :-\n",
    "\n",
    "$$ \\textbf{A}^{T}\\textbf{A} = \\textbf{A}\\textbf{A}^{T} = I $$\n",
    "\n",
    "This is a very important property as this implies that we can calculate the inverse of $\\textbf{A}$ very quickly ($\\textbf{A}^{-1} = \\textbf{A}^{T}$).\n",
    "\n",
    "#### Unitary Matrix\n",
    "\n",
    "#### Hermitian Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be covered in this chapter \n",
    "\n",
    "**Decomposition based on eigenvectors **\n",
    "\n",
    "- Eigendecomposition \n",
    "- Singular Value Decomposition\n",
    "\n",
    "**Decomposition related to solving system of linear equations **\n",
    "- LU decomposition\n",
    "- QR decomposition\n",
    "\n",
    "Sometimes breaking mathematical objects into parts leads to better undersanding of these objects. This also includes finding some properties of them which are universal irrespective of the way object is represented. As an example, it does not matter if we represent a number (say 12) using base 10 or any other base, it would be always true that $12 =  2\\times 2\\times 3$. \n",
    "\n",
    "The same is true for matrices, where we can decompose a matrix into parts which reveal information about their functional properties which is not clear prima facie. As an example, in LU decomposition, the matrix $\\textbf{A}$ is decomposed into two matrices $\\textbf{L}$ and $\\textbf{U}$ where $\\textbf{L}$ is an lower triangular matrix and $\\textbf{U}$ is an upper triangular matrix.\n",
    "\n",
    "$$ \\textbf{A}\\textbf{x} = (\\textbf{L}\\textbf{U})\\textbf{x} = \\textbf{b}$$\n",
    "$$ (\\textbf{L}\\textbf{U})\\textbf{x} = \\textbf{L}(\\textbf{U}\\textbf{x}) $$ \n",
    "$$ \\textbf{U}\\textbf{x} = (\\textbf{L})^{-1}\\textbf{b}$$\n",
    "\n",
    "This requires fewer additions and multiplications to solve the equation.\n",
    "\n",
    "\n",
    "One of the popular decomposition is called eigen-decomposition in which the matrix is broken down into other matrices containing eigenvectors and eigenvalues. As we can express a quadritic equation into matrix form, we can use the eigendecomposition of the matrix corresponding to quadritic equation to find it's maximum and minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decomposition based on eigenvectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition ( Spectral Decomposition )\n",
    "\n",
    "A eigenvector of matrix $\\textbf{A}$ is a vector $\\textbf{v}$ such that multiplication by $\\textbf{A}$ only changes the scale of $\\textbf{v}$.\n",
    "\n",
    "$$ \\textbf{A}\\textbf{v} = \\lambda \\textbf{v} $$\n",
    "\n",
    "Suppose that the matrix has $n$ independent eigenvectors. We can write the vectors as {$\\textbf{v}_{1}$, $\\textbf{v}_{2}$, $\\cdots$, $\\textbf{v}_{n}$} and the corresponding eigenvalues as {$\\lambda_{1}$, $\\lambda_{2}$, $\\cdots$, $\\lambda_{n}$}. The set of eigenvectors can be written as a matrix $\\textbf{V}$.\n",
    "\n",
    "$$ V = \\begin{bmatrix} \\textbf{v}_{1} & \\textbf{v}_{2}  & \\cdots & \\textbf{v}_{n} \\end{bmatrix}$$\n",
    "\n",
    "Similarly a matrix can be constructed for eigenvalues.\n",
    "\n",
    "$$ \\boldsymbol{\\lambda} = \\begin{bmatrix}  \\lambda_{1} & \\lambda_{2}  & \\cdots & \\lambda_{n}\\end{bmatrix}^{T}$$\n",
    "\n",
    "Then, the eigendecomposition of $\\textbf{A}$ is given by :-\n",
    "\n",
    "$$ \\textbf{A} = \\textbf{V}diag(\\boldsymbol{\\lambda})\\textbf{V}^{-1}$$\n",
    "\n",
    "**How can we decompose/factor the matrix in this way ?**\n",
    "\n",
    "The above equation can be easily understood if we think of $\\textbf{A}$ as a linear transformation and $\\textbf{V}$ as the change of basis matrix. Then, we can write :-\n",
    "\n",
    "$$ \\textbf{V}^{-1}\\textbf{A}\\textbf{V} = diag(\\lambda)$$\n",
    "\n",
    "where $\\textbf{A}$ is the linear transform in original space, $\\textbf{V}$ is the basis change matrix from eigenspace to original space  and $\\textbf{V}^{-1}$ is the basis change matrix from original space to the eigenspace. The resultant matrix is guaranteed to be diagonal because in the eigenspace of transform $\\textbf{A}$, the basis vectors consist of the eigenvectors which would only **scale**.\n",
    "\n",
    "**Are eigenvectors always orthogonal ?**\n",
    "\n",
    "The eigenvectors are orthogonal only if the matrix $\\textbf{A}$ is real symmetric matrix. This can be proven as the following :-\n",
    "\n",
    "First, as $\\textbf{A}$ is a symmetric matrix, we have \n",
    "\n",
    "$$ \\textbf{A} = \\textbf{A}^{T} $$\n",
    "\n",
    "Now, let's take any of the two eigenvectors $\\textbf{u}$ and $\\textbf{v}$ with the eigenvalues $\\lambda$ and $\\mu$ respectively. We have the following equations :-\n",
    "\n",
    "$$ \\textbf{A}\\textbf{u} = \\lambda\\textbf{u} $$\n",
    "$$ \\textbf{A}\\textbf{v} = \\mu\\textbf{v} $$\n",
    "\n",
    "If we can prove that $\\textbf{u}.\\textbf{v} = 0$, then it is proved that the eigenvectors are orthogonal. \n",
    "\n",
    "$$ \\boldsymbol{u.(Av) = u^{T}(Av) = u^{T}(\\mu v) = \\mu (u^{T}v) = \\mu (u.v)} --------- [1]$$\n",
    "$$ \\boldsymbol{(Au).v = (Au)^{T}.v = (\\lambda u)^{T}.v = \\lambda (u^{T}v) = \\lambda (u.v) } ------- [2]$$\n",
    "$$\\boldsymbol{(Au)^{T}.v = (u^{T}A^{T}).v = (u^{T}A).v = u^{T}.(Av)} ------- [3]$$\n",
    "\n",
    "Equation 3 implies that the each term in the first two equations are equal. This means that :-\n",
    "\n",
    "$$ \\lambda \\textbf{(u.v)} = \\mu \\textbf{(u.v)} $$\n",
    "\n",
    "This imples that either $\\mu=\\lambda$ or $\\textbf{u.v}=0$. The former is not possible as we have assumed that $\\mu$ and $\\lambda$ are two distinct eigenvalues.\n",
    "\n",
    "The primary effect of this property is that for change of basis matrix $\\textbf{V}$, every column is orthogonal to the other column. This implies that $\\textbf{V}\\textbf{V}^{T} = \\textbf{I}$. Thus, we can write the eigendecomposition of the a real symmytric matrix as :-\n",
    "\n",
    "$$ \\textbf{A} = \\textbf{V} diag({\\lambda}) \\textbf{V}^{T}$$\n",
    "\n",
    "This makes it easier to find the inverse of the transformation basis. Hence, we can decompose a real symmetric matrix faster than any the matrix which is not real-symmetric.\n",
    "\n",
    "**On uniqueness of eigendecomposition**\n",
    "\n",
    "**Why do we need a square matrix for eigendecomposition ?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application ( Quadritic Form Optimisation )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "> Singular Value Decomposition is not nearly as famous as it should be - Gilbert Strang\n",
    "\n",
    "In the last section, we found out that eigendecomposition worked for square matrices only. In singular value decomposition, we factor the matrices into three different matrices. \n",
    "\n",
    "$$ \\textbf{A} = \\textbf{U}\\textbf{D}\\textbf{V}^{T} $$\n",
    "\n",
    "where $\\textbf{U}$ is the matrix whose columns contain left singular vectors, $\\textbf{D}$ is the diagonal matrix containing singular values and $\\textbf{V}$ is the matrix whose columns contain right singular values. The matrices $\\textbf{U}$ and $\\textbf{V}$ are orthogonal matrices.\n",
    "\n",
    "**How can we decompose/factor the matrix in this way ?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application ( Surveillence Video Background Removal )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application ( Pseudoinverse )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation between SVD and EigenValue Decomposition\n",
    "\n",
    "The SVD is very general in the sense that it can be applied to any $m \\times n$ matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrix. Given the SVD of a matrix A, the following relations hold :-\n",
    "\n",
    "$$\\textbf{A}^{T}\\textbf{A} = \\textbf{V}\\textbf{D}^{T}\\textbf{U}^{T} \\textbf{U}\\textbf{D}\\textbf{V}^{T}$$\n",
    "\n",
    "As $\\textbf{U}$ is an orthogonal matrix $\\textbf{U}\\textbf{U}^{T} = \\textbf{I}$\n",
    "\n",
    "$$ \\implies \\textbf{A}^{T}\\textbf{A} = \\textbf{V}(\\textbf{D}^{T} \\textbf{D})\\textbf{V}^{T}$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$ \\textbf{A}\\textbf{A}^{T} = \\textbf{U}(\\textbf{D}\\textbf{D}^{T})\\textbf{U}^{T} $$\n",
    "\n",
    "This implies that columns of $\\textbf{V}$ are eigenvectors of $\\textbf{A}^{T}\\textbf{A}$, the columns of $\\textbf{U}$ are eigenvectors of $\\textbf{A}\\textbf{A}^{T}$ and the non-zero elements of $\\textbf{D}$ are the square roots of the eigenvalues of both $\\textbf{A}\\textbf{A}^{T}$ and  $\\textbf{A}^{T}\\textbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decomposition related to solving the system of linear equations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LU Decomposition\n",
    "\n",
    "In numerical analysis and linear algebra, LU decomposition is used to factor a matrix into two matrices $\\textbf{L}$ and $\\textbf{U}$ which are lower triangular and upper triangluar matrices respectively. \n",
    "\n",
    "It helps in the following :-\n",
    "\n",
    "- Solving a system of linear equations through Gaussian elimination\n",
    "- Finding inverse of a matrix\n",
    "- Computing the determinant of a matrix\n",
    "\n",
    "$$\\textbf{A} = \\textbf{L}\\textbf{U}$$\n",
    "\n",
    "$$\\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\  a_{21} & a_{22} & a_{23} \\\\  a_{31} & a_{32} & a_{33} \\end{bmatrix}  = \\begin{bmatrix} l_{11} & 0 & 0 \\\\  l_{21} & l_{22} & 0 \\\\  l_{31} & a_{32} & a_{33} \\end{bmatrix} \\begin{bmatrix} u_{11} & u_{12} & u_{13} \\\\  u_{21} & u_{22} & 0 \\\\  u_{31} & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "**What is the use of LU decomposition ?**\n",
    "\n",
    "It summerises the Gaussian elimination in matrix form.\n",
    "\n",
    "**What are the conditions for LU decomposition to exist ?**\n",
    "\n",
    "- A should be a square matrix \n",
    "- Permuation matrix should be 0\n",
    "\n",
    "Exapanding on the last point, it is not always possible to decompose the matrix into a lower triangular and upper triangular matrix, say if $a_{11}=0$ in original matrix then either $u_{11}$ or $l_{11}$ has to be 0. This means that either $\\textbf{L}$ or $\\textbf{U}$ should be non-invertible (All 0's in one row). However, this is not possible as  $\\textbf{A}$ is invertible. This problem can be solved by just reordering the rows of $\\textbf{A}$ so that the first element of permuted matrix is not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
