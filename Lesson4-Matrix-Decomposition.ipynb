{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Decomposition - Eigendecomposition and Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be covered in this chapter \n",
    "\n",
    "**Decomposition based on eigenvectors **\n",
    "\n",
    "- Eigendecomposition \n",
    "- Singular Value Decomposition\n",
    "\n",
    "**Decomposition related to solving system of linear equations **\n",
    "- LU decomposition\n",
    "- QR decomposition\n",
    "\n",
    "Sometimes breaking mathematical objects into parts leads to better undersanding of these objects. This also includes finding some properties of them which are universal irrespective of the way object is represented. As an example, it does not matter if we represent a number (say 12) using base 10 or any other base, it would be always true that $12 =  2\\times 2\\times 3$. \n",
    "\n",
    "The same is true for matrices, where we can decompose a matrix into parts which reveal information about their functional properties which is not clear prima facie. As an example, in LU decomposition, the matrix $\\textbf{A}$ is decomposed into two matrices $\\textbf{L}$ and $\\textbf{U}$ where $\\textbf{L}$ is an lower triangular matrix and $\\textbf{U}$ is an upper triangular matrix.\n",
    "\n",
    "$$ \\textbf{A}\\textbf{x} = (\\textbf{L}\\textbf{U})\\textbf{x} = \\textbf{b}$$\n",
    "$$ (\\textbf{L}\\textbf{U})\\textbf{x} = \\textbf{L}(\\textbf{U}\\textbf{x}) $$ \n",
    "$$ \\textbf{U}\\textbf{x} = (\\textbf{L})^{-1}\\textbf{b}$$\n",
    "\n",
    "This requires fewer additions and multiplications to solve the equation.\n",
    "\n",
    "\n",
    "One of the popular decomposition is called eigen-decomposition in which the matrix is broken down into other matrices containing eigenvectors and eigenvalues. As we can express a quadritic equation into matrix form, we can use the eigendecomposition of the matrix corresponding to quadritic equation to find it's maximum and minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decomposition based on eigenvectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition ( Spectral Decomposition )\n",
    "\n",
    "A eigenvector of matrix $\\textbf{A}$ is a vector $\\textbf{v}$ such that multiplication by $\\textbf{A}$ only changes the scale of $\\textbf{v}$.\n",
    "\n",
    "$$ \\textbf{A}\\textbf{v} = \\lambda \\textbf{v} $$\n",
    "\n",
    "Suppose that the matrix has $n$ independent eigenvectors. We can write the vectors as {$\\textbf{v}_{1}$, $\\textbf{v}_{2}$, $\\cdots$, $\\textbf{v}_{n}$} and the corresponding eigenvalues as {$\\lambda_{1}$, $\\lambda_{2}$, $\\cdots$, $\\lambda_{n}$}. The set of eigenvectors can be written as a matrix $\\textbf{V}$.\n",
    "\n",
    "$$ V = \\begin{bmatrix} \\textbf{v}_{1} & \\textbf{v}_{2}  & \\cdots & \\textbf{v}_{n} \\end{bmatrix}$$\n",
    "\n",
    "Similarly a matrix can be constructed for eigenvalues.\n",
    "\n",
    "$$ \\boldsymbol{\\lambda} = \\begin{bmatrix}  \\lambda_{1} & \\lambda_{2}  & \\cdots & \\lambda_{n}\\end{bmatrix}^{T}$$\n",
    "\n",
    "Then, the eigendecomposition of $\\textbf{A}$ is given by :-\n",
    "\n",
    "$$ \\textbf{A} = \\textbf{V}diag(\\boldsymbol{\\lambda})\\textbf{V}^{-1}$$\n",
    "\n",
    "**How can we decompose/factor the matrix in this way ?**\n",
    "\n",
    "The above equation can be easily understood if we think of $\\textbf{A}$ as a linear transformation and $\\textbf{V}$ as the change of basis matrix. Then, we can write :-\n",
    "\n",
    "$$ \\textbf{V}^{-1}\\textbf{A}\\textbf{V} = diag(\\lambda)$$\n",
    "\n",
    "where $\\textbf{A}$ is the linear transform in original space, $\\textbf{V}$ is the basis change matrix from eigenspace to original space  and $\\textbf{V}^{-1}$ is the basis change matrix from original space to the eigenspace. The resultant matrix is guaranteed to be diagonal because in the eigenspace of transform $\\textbf{A}$, the basis vectors consist of the eigenvectors which would only **scale**.\n",
    "\n",
    "**Are eigenvectors always orthogonal ?**\n",
    "\n",
    "The eigenvectors are orthogonal only if the matrix $\\textbf{A}$ is real symmetric matrix. This can be proven as the following :-\n",
    "\n",
    "First, as $\\textbf{A}$ is a symmetric matrix, we have \n",
    "\n",
    "$$ \\textbf{A} = \\textbf{A}^{T} $$\n",
    "\n",
    "Now, let's take any of the two eigenvectors $\\textbf{u}$ and $\\textbf{v}$ with the eigenvalues $\\lambda$ and $\\mu$ respectively. We have the following equations :-\n",
    "\n",
    "$$ \\textbf{A}\\textbf{u} = \\lambda\\textbf{u} $$\n",
    "$$ \\textbf{A}\\textbf{v} = \\mu\\textbf{v} $$\n",
    "\n",
    "If we can prove that $\\textbf{u}.\\textbf{v} = 0$, then it is proved that the eigenvectors are orthogonal. \n",
    "\n",
    "$$ \\boldsymbol{u.(Av) = u^{T}(Av) = u^{T}(\\mu v) = \\mu (u^{T}v) = \\mu (u.v)} --------- [1]$$\n",
    "$$ \\boldsymbol{(Au).v = (Au)^{T}.v = (\\lambda u)^{T}.v = \\lambda (u^{T}v) = \\lambda (u.v) } ------- [2]$$\n",
    "$$\\boldsymbol{(Au)^{T}.v = (u^{T}A^{T}).v = (u^{T}A).v = u^{T}.(Av)} ------- [3]$$\n",
    "\n",
    "Equation 3 implies that the each term in the first two equations are equal. This means that :-\n",
    "\n",
    "$$ \\lambda \\textbf{(u.v)} = \\mu \\textbf{(u.v)} $$\n",
    "\n",
    "This imples that either $\\mu=\\lambda$ or $\\textbf{u.v}=0$. The former is not possible as we have assumed that $\\mu$ and $\\lambda$ are two distinct eigenvalues.\n",
    "\n",
    "The primary effect of this property is that for change of basis matrix $\\textbf{V}$, every column is orthogonal to the other column. This implies that $\\textbf{V}\\textbf{V}^{T} = \\textbf{I}$. Thus, we can write the eigendecomposition of the a real symmytric matrix as :-\n",
    "\n",
    "$$ \\textbf{A} = \\textbf{V} diag({\\lambda}) \\textbf{V}^{T}$$\n",
    "\n",
    "This makes it easier to find the inverse of the transformation basis. Hence, we can decompose a real symmetric matrix faster than any the matrix which is not real-symmetric.\n",
    "\n",
    "**On uniqueness of eigendecomposition**\n",
    "\n",
    "**Why do we need a square matrix for eigendecomposition ?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application ( Quadritic Form Optimisation )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "> Singular Value Decomposition is not nearly as famous as it should be - Gilbert Strang\n",
    "\n",
    "In the last section, we found out that eigendecomposition worked for square matrices only. In singular value decomposition, we factor the matrices into three different matrices. \n",
    "\n",
    "$$ \\textbf{A} = \\textbf{U}\\textbf{D}\\textbf{V}^{T} $$\n",
    "\n",
    "where $\\textbf{U}$ is the matrix whose columns contain left singular vectors, $\\textbf{D}$ is the diagonal matrix containing singular values and $\\textbf{V}$ is the matrix whose columns contain right singular values. The matrices $\\textbf{U}$ and $\\textbf{V}$ are orthogonal matrices.\n",
    "\n",
    "**How can we decompose/factor the matrix in this way ?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application ( Surveillance  Video Background Removal )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application ( Pseudoinverse )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation between SVD and EigenValue Decomposition\n",
    "\n",
    "The SVD is very general in the sense that it can be applied to any $m \\times n$ matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrix. Given the SVD of a matrix A, the following relations hold :-\n",
    "\n",
    "$$\\textbf{A}^{T}\\textbf{A} = \\textbf{V}\\textbf{D}^{T}\\textbf{U}^{T} \\textbf{U}\\textbf{D}\\textbf{V}^{T}$$\n",
    "\n",
    "As $\\textbf{U}$ is an orthogonal matrix $\\textbf{U}\\textbf{U}^{T} = \\textbf{I}$\n",
    "\n",
    "$$ \\implies \\textbf{A}^{T}\\textbf{A} = \\textbf{V}(\\textbf{D}^{T} \\textbf{D})\\textbf{V}^{T}$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$ \\textbf{A}\\textbf{A}^{T} = \\textbf{U}(\\textbf{D}\\textbf{D}^{T})\\textbf{U}^{T} $$\n",
    "\n",
    "This implies that columns of $\\textbf{V}$ are eigenvectors of $\\textbf{A}^{T}\\textbf{A}$, the columns of $\\textbf{U}$ are eigenvectors of $\\textbf{A}\\textbf{A}^{T}$ and the non-zero elements of $\\textbf{D}$ are the square roots of the eigenvalues of both $\\textbf{A}\\textbf{A}^{T}$ and  $\\textbf{A}^{T}\\textbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decomposition related to solving the system of linear equations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
