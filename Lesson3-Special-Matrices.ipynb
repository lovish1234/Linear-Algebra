{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that a linear equation cannot have two solutions. (Eigher one or infinitely many solutions) ??\n",
    "\n",
    "Think of the columns of $\\textbf{A}$ as the different directions we can travel to from origin, then determine how many different ways we can follow to reach $\\textbf{b}$.\n",
    "\n",
    "$$ \\textbf{A}\\textbf{x} = \\sum_{i}x_{i}\\textbf{A}_{:,i}$$\n",
    "\n",
    "If $\\textbf{b}$ is in the span of the coumns of $\\textbf{A}$, then $\\textbf{A}\\textbf{x}=\\textbf{b}$ must have  a solution. In this particular case, the span is called the range or **column space** of $\\textbf{A}$.\n",
    "\n",
    "The requirement that column space of $\\textbf{A}$ be all of $\\mathbb{R}^{m}$ implies that $\\textbf{A}$ must have atleast m columns. Otherwise dimensionalty of column space would be less than m. A an example take $\\textbf{A}$ as a $3 \\times 2$ matrix, that means we only have two $x_{i}$'s which can vary to produce the output. The output will lie on a 2-D plane in the 3-D space. However, this is necessary and not sufficient condition for $\\textbf{A}\\textbf{x}=\\textbf{b}$ having a solution.\n",
    "\n",
    "Even if the matrix has 3 columns, if two of them are the same, output for $\\textbf{A}\\textbf{x}$ would lie on $\\mathbb{R}^{2}$. Thus the columns should also be linearly independent.\n",
    "\n",
    "For the matrix $\\textbf{A}$ to have inverse, it is required that the equation has exactly one solution for each value of $\\textbf{b}$. Thus the matrix should have **m linearly independent columns**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Gauss-Jordan Elimination to find Inverse**\n",
    "\n",
    "$$ \\textbf{E}\\begin{bmatrix} \\textbf{A} & \\textbf{I}\\end{bmatrix} = \\begin{bmatrix} \\textbf{I} & \\textbf{A} \\end{bmatrix}$$ \n",
    "\n",
    "where $\\textbf{E}$ is the tranformation matrix which converts the matrix $\\textbf{A}$ to $\\textbf{I}$ ie. it is $\\textbf{A}^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norms are the set of functions mapping vectros to a set of non-negative values. The norm of a vector $\\textbf{x}$ measures the distance between origin to the point x in the space.\n",
    "\n",
    "$$ ||\\textbf{x}||_{p} = \\Big(\\sum_{i}|x_{i}|^{p}\\Big)^{\\frac{1}{p}}$$\n",
    "\n",
    "Formally, norm us any function that satisfies the following properties :-\n",
    "\n",
    "- f(**x**)=0 $\\implies$ x=0\n",
    "- f(**x**+**y**) $\\leq$ f(**x**)+f(**y**) (triangle inequality)\n",
    "- $\\forall \\alpha \\in \\mathbb{R}$, f($\\alpha$**x**)=$\\alpha$f(**x**) \n",
    "\n",
    "Most common norm is the $L^{2}(\\textbf{x})$ norm, also denoted as $||\\textbf{x}||$. $L^{2}(\\textbf{x})$ norm grows very slowly near the origin, hence when we need the distance value to increase quickly near the origin we use $L^{1}(\\textbf{x})$ norm. This is very important for some problems in machine learning where we need to differnciate the elements which are exactly 0 with other which are near to 0. $L^{1}(\\textbf{x})$ norm grows at the same rate in all the locations. \n",
    "\n",
    "There are other commonly used norms defined as following :-\n",
    "\n",
    "- $L^{0}(\\textbf{x})$ norm - Number of non-zero elements in a vector $\\textbf{x}$. Actually this violates the third requirement of being a norm. Still, it is mis-named as a norm.\n",
    "- $L^{\\infty}(\\textbf{x})$ norm - Max element in the vector $\\textbf{x}$\n",
    "$$||\\textbf{x}||_{\\infty} = max_{i} |x_{i}| $$\n",
    "- Frobenius Norm - $L^{2}$ norm with a martix $\\textbf{A}$ instead of a vector $\\textbf{x}$\n",
    "$$ ||\\textbf{A}||_{2} = \\sqrt{\\Big(\\sum_{i}\\sum_{j}|A_{i,j}|^{2}\\Big)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Kind of Matrices \n",
    "\n",
    "#### Upper Triangular Matrix\n",
    "\n",
    "#### Lower Triangular Matrix\n",
    "\n",
    "#### Diagonal Matrices\n",
    "\n",
    "A diagonal matrix, written as $\\textbf{D}$ is a matrix in which the individual entries are defined as follows :-\n",
    "\n",
    "$$\\textbf{D}_{i,j} \\neq 0 \\hspace{0.1in} \\forall i,j : i\\neq j$$\n",
    "$$\\textbf{D}_{i,j} = 0 \\hspace{0.1in} \\forall i,j : i=j$$\n",
    "\n",
    "This defination leads to many interesting properties as $\\textbf{D}\\textbf{x} = \\textbf{D}\\odot\\textbf{x}$. This means multiplying a diagonal matrix with another vector just requires the pointwise multiplication of the corresponding elements. Also, inverting a diagonal matrix requries less number of operations as \n",
    "\n",
    "$$\\textbf{D}^{-1} = \\begin{bmatrix} \\frac{1}{d_{1}} & \\dotsc & \\dotsc  & \\dotsc \\\\ \\dotsc & \\frac{1}{d_{2}} & \\dotsc  & \\dotsc \\\\ \\dotsc & \\dotsc & \\frac{1}{d_{i}} & \\dotsc \\\\ \\dotsc & \\dotsc & \\dotsc  & \\frac{1}{d_{n}} \\end{bmatrix}$$\n",
    "\n",
    "Diagnomal matrix need not be square. Although, non-square diagonal matrix does not have inverse, a multiplication by these matrices is still cheap. If number of rows is greater than number of columns :-\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 0  \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "concatenate zeros to the result. If the number of columns are greater than number of rows :-\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0  \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix} = \\begin{bmatrix} x_{1} \\\\ x_{2}  \\end{bmatrix}$$\n",
    "\n",
    "discard last elements of $\\textbf{x}$\n",
    "\n",
    "\n",
    "\n",
    "In Machine Learning, restricting some matrices to daigonal may result in a more efficient algorithm\n",
    "\n",
    "#### Symmetric Matrix\n",
    "\n",
    "A matrix which is equivalent to it's own tranpose is called symmetric matrix. \n",
    "\n",
    "$$ \\textbf{A} = \\textbf{A}^{T}$$\n",
    "\n",
    "In machine learning, symmetric matrix appear when it's entries are generated by the function having two arguments in which order of the arguments have no effect on the output of the function eg. distance metric\n",
    "\n",
    "#### Skew-Symmytric Matrix \n",
    "\n",
    "\n",
    "#### Orthogonal/Orthonormal Matrices \n",
    "\n",
    "A vector $\\textbf{x}$ and $\\textbf{y}$ are called orthogonal if $\\textbf{x}^{T}\\textbf{y}=0$. If both of these vectors have non-zero norm, it means that they are at 90 degrees angle to each other. If both of the vectors have a unit norm, they are called **orthonormal**.\n",
    "\n",
    "Orthonormal matrix is a square matrix in which both the rows and columns are mutually orthonormal to each other. For this reason :-\n",
    "\n",
    "$$ \\textbf{A}^{T}\\textbf{A} = \\textbf{A}\\textbf{A}^{T} = I $$\n",
    "\n",
    "This is a very important property as this implies that we can calculate the inverse of $\\textbf{A}$ very quickly ($\\textbf{A}^{-1} = \\textbf{A}^{T}$).\n",
    "\n",
    "#### Unitary Matrix\n",
    "\n",
    "#### Hermitian Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
